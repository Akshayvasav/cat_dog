#!/usr/bin/env python
# coding: utf-8

# In[1]:


import cv2
import os
import pathlib
import numpy as np
from tqdm import tqdm
import pandas


# In[2]:


class_file = r"C:\Users\201039530\OneDrive - Alstom\labelImg-master\data\predefined_classes.txt"
#gt_txt_path = r"C:\Users\201039530\OneDrive - Alstom\Annotations\Track_layout_Automation\signaling_elements\TrackLayoutImages_SignalingElements\Big_component_modified_component\labels\valid\2_1.txt"
#dt_txt_path = r"C:\Users\201039530\OneDrive - Alstom\yolov5_changed_eval\runs\detect\Track_Layout_Automation_big_signaling_elements_imgz_1408_yolov5l_modified_annotations_10_classes\exp4\labels\2_1.txt"
#source_dir = './data/'
pred_dir = r"C:\Users\201039530\OneDrive - Alstom\yolov3-master\runs\detect\Track_Layout_Automation_signaling_elements_blockwise_21_classes_modified_annotations_yolov5l\exp\labels"
source_dir = r"C:\Users\201039530\OneDrive - Alstom\Annotations\Track_layout_Automation\signaling_elements\TrackLayoutImages_SignalingElements\blockwise_split_images_labels_608\labels\valid"
excel_name = "TLASE_blockwise_yolov3_imgz_608_21_classes_yolov5l_modified_annotations"
join = os.path.join
save_dir = r"C:\Users\201039530\OneDrive - Alstom\Track_Layout_Automation\comparison_yolov3_vs_yolov5\Performance_Metrics\yolov3"
width = 3570
height = 2523


# In[3]:


labels = []

def load_class(class_file):
    global labels
    file = open(class_file)
    labels = [x.strip() for x in file.readlines()]
    labels.append('_negative_')


def load_annotation_from_txt(file_path, gt = False):
    with open(file_path, 'r') as f:
        annotations = f.readlines()
        if gt:
            annotations = process_annotations(annotations)
    f.close()
    #print(annotations)
    return annotations

def process_annotations(annotations):
    clean_annotations = []
    for annotation in annotations:
        annotation = annotation.strip()
        annotation = [float(item) for item in annotation.split()]
        annotation[0] = int(annotation[0])
        clean_annotations.append(annotation)
    return clean_annotations


def get_absolute_values(ann):
    label, x_norm, y_norm, w_norm, h_norm = ann
    # extract absolute values from normalised values
    x_cen = int(x_norm * width)
    y_cen = int(y_norm * height)
    box_width = int(w_norm * width)
    box_height = int(h_norm * height)
    # calc absolute x, y (top left)
    x = x_cen - int(box_width/2)
    y = y_cen - int(box_height/2)
    return(labels[label], x, y, box_width, box_height)

def load_ground_truth(path):
    annotations = load_annotation_from_txt(path, gt = True)
    annotations = [get_absolute_values(obj) for obj in annotations] # annotations = [[label, x, y, w, h]]
    return annotations


def load_predictions(path, conf_thresh = 0.5):
    dt_df = pandas.read_csv(path, sep=" ", names=['class_id_dt','x_cen','y_cen','width','height','confidence'])
    dt_df = dt_df.sort_values(by=['class_id_dt'],ignore_index=True)
    dt_df = dt_df.loc[:,['class_id_dt','confidence','x_cen','y_cen','width','height']]
    dt_df['x_cen']= (dt_df['x_cen'] * width) - ((dt_df['width']* width) /2)
    dt_df['y_cen']= (dt_df['y_cen'] * height) - ((dt_df['height']* height)/2)
    dt_df['width']= (dt_df['width'] * width)
    dt_df['height']= (dt_df['height'] * height)
    dt_df['x_cen'] = dt_df['x_cen'].astype(int)
    dt_df['y_cen'] = dt_df['y_cen'].astype(int)
    dt_df['width'] = dt_df['width'].astype(int)
    dt_df['height'] = dt_df['height'].astype(int)
    dt_df = dt_df[dt_df['confidence'] >= conf_thresh]
    for i in range(dt_df.shape[0]):
        dt_df['class_id_dt'].iloc[i] = labels[dt_df['class_id_dt'].iloc[i]]
    predictions_processed = list(dt_df.itertuples(index=False, name=None))
    return predictions_processed

def bb_intersection_over_union(boxA, boxB):
    # determine the (x, y)-coordinates of the intersection rectangle
    xA = max(boxA[0], boxB[0])
    yA = max(boxA[1], boxB[1])
    xB = min(boxA[2], boxB[2])
    yB = min(boxA[3], boxB[3])
    # compute the area of intersection rectangle
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)
    # compute the area of both the prediction and ground-truth
    # rectangles
    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)
    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)
    # compute the intersection over union by taking the intersection
    # area and dividing it by the sum of prediction + ground-truth
    # areas - the interesection area
    iou = interArea / float(boxAArea + boxBArea - interArea)
    # return the intersection over union value
    return iou

def calculate_results(source_dir, pred_dir, iou_threshold):
    results = []
    for file in tqdm(os.listdir(source_dir)):
        #print(os.listdir(source_dir))
        try:
            if file.endswith('txt'):
                annotations = load_ground_truth(join(source_dir, file))
                #print(source_dir)
                predictions_path = join(pred_dir, file)
                #print(predictions_path)
                predictions = load_predictions(predictions_path,conf_thresh = 0.5)

                # results specific to current file
                file_results = []

                # if predictions/annotations are empty, add _negative_ label and zero bb

                # for ground truth, gts = ground truths
                if len(annotations) > 0:
                    gts = [[label, [x, y, x+w, y+h]] for label, x, y, w, h in annotations]
                else:
                    gts = [['_negative_', [0, 0, 0, 0]]]

                # for predictions, dts = detections
                if len(predictions) > 0:
                    dts = [[label, [x, y, x + w, y + h]] for label, conf, x, y, w, h in predictions]
                else:
                    dts = [['_negative_', [0, 0, 0, 0]]]

                # calculate iou for gt bbox against pred bbox
                for gt_label, gt_bbox in gts:
                    for dt_label, dt_bbox in dts:
                        iou = bb_intersection_over_union(gt_bbox, dt_bbox)
                        if (iou > iou_threshold) & (gt_label != '_negative_') & (dt_label != '_negative_'):
                            file_results.append([file, gt_label, gt_bbox, dt_label, dt_bbox, iou, dt_label == gt_label])

                # get the bboxes added for gt and pred in file_results
                if len(file_results) > 0:
                    file_results_np = np.array(file_results, dtype = object)
                    added_gt_bboxes = file_results_np[:, 2].tolist()
                    added_dt_bboxes = file_results_np[:, 4].tolist()
                else:
                    added_gt_bboxes = []
                    added_dt_bboxes = []

                # if annotation is not added in results, append it in results
                for label, bbox in gts:
                    if bbox not in added_gt_bboxes:
                        file_results.append([file, label, bbox, '_negative_', [0, 0, 0, 0], 0.0, False])

                # if prediction is not added in results, append it in results
                for label, bbox in dts:
                    if bbox not in added_dt_bboxes:
                        file_results.append([file, '_negative_', [0, 0, 0, 0], label, bbox, 0.0, False])
                [results.append(x) for x in file_results]
                print(" Done for", file)

        except Exception as e:
            #print("Exception",e)
            if not os.path.isfile(file):
                print('missing', file)
            else:
                print(file)
        
        # load the results array to pandas dataframe
        df = pandas.DataFrame()
        df = pandas.DataFrame.from_records(results, columns = ['file', 'gt_label', 'gt_bbox', 'pred_label', 'pred_bbox', 'iou', 'valid'])
        df.reset_index(inplace=True)
        
    return df


# ## export data to pandas dataframe


# ## Write results to excel sheet

def create_excel_writer(excel_name, save_dir):
    # excel_name = basename(pred_dir[:-1]) if pred_dir.endswith('/') else basename(pred_dir)
    # excel_save_dir = pred_dir
    excel_dir = join(save_dir, excel_name + '.xlsx')
    writer = pandas.ExcelWriter(excel_dir, engine = 'xlsxwriter')
    return writer


# ## Calculate Precision and Recall


def calculate_precision_recall(df, writer):
    pr_data = []
    for label in labels:
        try:
            tp = len(df[(df['gt_label'] == label) & (df['valid'] == True)])
            fn = len(df[(df['gt_label'] == label) & (df['valid'] == False)])
            fp = len(df[(df['pred_label'] == label) & (df['valid'] == False)])
            gt = len(df[df['gt_label'] == label])
            accuracy = tp/gt*100
            precision = tp/(tp + fp)
            recall = tp/(tp + fn)
            f1 = 2 * ((precision * recall)/(precision + recall))
            iou = df[(df['gt_label'] == label) & (df['valid'] == True)]['iou'].mean()
            #print(label, 'iou: ', iou)
            pr_data.append([label, tp, fp, fn, precision, recall, f1, iou, accuracy, gt])
            #image_wise_result_df.append
        except:
            # if divide by zero error occurs append label, tp, fp, fn and 0 for (precision, recall)
            # happens when there are no tp
            pr_data.append([label, tp, fp, fn, 0, 0, 0, 0, 0, gt])

    precision_recall = pandas.DataFrame.from_records(pr_data, columns = ['class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1 score', 'iou', 'accuracy', 'ground truth'])
    precision_recall.to_excel(writer, sheet_name = 'metrics', index = False)
    return precision_recall
    
def calculate_confusion_matrix(df, writer):
    cf_records = []
    for gt_label in labels:
        row = []
        row.append(gt_label)
        #print(gt_label)
        for pred_label in labels:
            row.append(len(df[(df['gt_label'] == gt_label) & (df['pred_label'] == pred_label)]))
        cf_records.append(row)

    confusion_matrix = pandas.DataFrame.from_records(cf_records, columns = [' ', *labels])
    confusion_matrix.to_excel(writer, sheet_name = 'confusion matrix', index = False)
    return confusion_matrix
def imagewise_metrixs(df,writer):
    im_data = []
    for i in range(len(df['file'].unique())):
        #print(((df['file'].unique()))[i])
        df_per_image = df.loc[list(np.where(df["file"] == list((df['file'].unique()))[i]))[0][0]:list(np.where(df["file"] == list((df['file'].unique()))[i]))[0][-1],]
        #print(((df['file'].unique()))[i],"---",df_per_image.shape)
        for label in labels:
            try:
                #print(label)
                tp = len(df_per_image[(df_per_image['gt_label'] == label) & (df_per_image['valid'] == True)])
                fn = len(df_per_image[(df_per_image['gt_label'] == label) & (df_per_image['valid'] == False)])
                fp = len(df_per_image[(df_per_image['pred_label'] == label) & (df_per_image['valid'] == False)])
                gt = len(df_per_image[df_per_image['gt_label'] == label])
                accuracy = tp/gt*100
                #precision = tp/(tp + fp)
                #recall = tp/(tp + fn)
                #f1 = 2 * ((precision * recall)/(precision + recall))
                #iou = df_per_image[(df_per_image['gt_label'] == label) & (df_per_image['valid'] == True)]['iou'].mean()
                #im_data.append([label, tp, fp, fn, precision, recall, f1, iou, accuracy, gt])
                im_data.append([list((df['file'].unique()))[i],label, tp, fp, fn, gt, accuracy])
                #print(im_data)
                #precision_recall_imagewise = pandas.DataFrame.from_records(im_data, columns = ['class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1 score', 'iou', 'accuracy', 'ground truth'])
                #precision_recall_imagewise = pandas.DataFrame.from_records(im_data, columns = ['class', 'TP', 'FP', 'FN'])
                #print(precision_recall_imagewise)
            except:
                # if divide by zero error occurs append label, tp, fp, fn and 0 for (precision, recall)
                # happens when there are no tp
                #im_data.append([label, tp, fp, fn, 0, 0, 0, 0, 0, gt])
                im_data.append([list((df['file'].unique()))[i],label, tp, fp, fn, gt, accuracy])
                #precision_recall_imagewise = pandas.DataFrame.from_records(im_data, columns = ['class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1 score', 'iou', 'accuracy', 'ground truth'])
            precision_recall_imagewise = pandas.DataFrame.from_records(im_data, columns = ['file_name','class', 'TP', 'FP', 'FN', 'GT', 'Accuracy'])
        #precision_recall_imagewise.to_excel(writer, sheet_name = 'precision_recall_imagewise', index = False)
    #precision_recall_imagewise
    #writer = create_excel_writer("precision_recall_imagewise", save_dir)
    precision_recall_imagewise.to_excel(writer, sheet_name = 'precision_recall_imagewise', index = False)
    #precision_recall_imagewise.to_csv(file_name)
    return precision_recall_imagewise


# In[4]:


#load_class(class_file)


# In[5]:


#labels


# In[6]:


#load_annotation_from_txt(gt_txt_path)


# In[7]:


#load_predictions(dt_txt_path)


# In[8]:


#load_ground_truth(join(source_dir, "8_1.txt"))


# In[9]:


def process_results(source_dir, pred_dir, excel_name, iou_threshold = 0.4):
    
    if not os.path.isdir(source_dir):
        print('SOURCE DIR DOES NOT EXIST', source_dir)
        return
    if not os.path.isdir(pred_dir):
        print('PRED DIR DOES NOT EXIST', pred_dir)
        return

    writer = create_excel_writer(excel_name, save_dir)

    results = calculate_results(source_dir, pred_dir, iou_threshold)

    results.to_excel(writer, 'results')

    precision_recall = calculate_precision_recall(results, writer)

    confusion_matrix = calculate_confusion_matrix(results, writer)
    
    precision_recall_imagewise = imagewise_metrixs(results, writer)

    writer.save()
    
    return {'confusion_matrix': confusion_matrix, 'precision_recall': precision_recall}    


# In[10]:


process_results(source_dir, pred_dir, excel_name)


# In[ ]:





# In[ ]:




